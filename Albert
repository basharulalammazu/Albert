{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7316566,"sourceType":"datasetVersion","datasetId":4245661}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install contractions\n!pip install autocorrect\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport re\nimport string\nimport emoji\nimport nltk\nimport contractions\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.corpus import words\nfrom autocorrect import Speller\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport torch\nimport numpy as np\nfrom transformers import AlbertTokenizer\nfrom transformers import AlbertForSequenceClassification # For Load model (AlbertForSequenceClassification)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\nfrom transformers import Trainer, TrainingArguments, AlbertForSequenceClassification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:55:57.336868Z","iopub.execute_input":"2025-06-10T15:55:57.337193Z","iopub.status.idle":"2025-06-10T15:56:40.583563Z","shell.execute_reply.started":"2025-06-10T15:55:57.337166Z","shell.execute_reply":"2025-06-10T15:56:40.582838Z"}},"outputs":[{"name":"stdout","text":"Collecting contractions\n  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting textsearch>=0.0.21 (from contractions)\n  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\nCollecting anyascii (from textsearch>=0.0.21->contractions)\n  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\nCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nDownloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\nDownloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\nDownloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\nSuccessfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\nCollecting autocorrect\n  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: autocorrect\n  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=c5759bcce22aa4928e6c1b484f7c90cabd1d32bd97adf94f3d6e1d05b227d976\n  Stored in directory: /root/.cache/pip/wheels/5e/90/99/807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\nSuccessfully built autocorrect\nInstalling collected packages: autocorrect\nSuccessfully installed autocorrect-2.6.1\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749570984.584493      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749570984.636989      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Initialize tools\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\nspell = Speller(lang='en')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.584972Z","iopub.execute_input":"2025-06-10T15:56:40.585874Z","iopub.status.idle":"2025-06-10T15:56:40.880479Z","shell.execute_reply.started":"2025-06-10T15:56:40.585845Z","shell.execute_reply":"2025-06-10T15:56:40.879571Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Convert to Lowercase","metadata":{}},{"cell_type":"code","source":"def to_lowercase(text):\n    return text.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.881463Z","iopub.execute_input":"2025-06-10T15:56:40.881788Z","iopub.status.idle":"2025-06-10T15:56:40.886420Z","shell.execute_reply.started":"2025-06-10T15:56:40.881761Z","shell.execute_reply":"2025-06-10T15:56:40.885514Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Remove Numbers","metadata":{}},{"cell_type":"code","source":"def remove_numbers(text):\n    return re.sub(r'\\d+', '', text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.888643Z","iopub.execute_input":"2025-06-10T15:56:40.889200Z","iopub.status.idle":"2025-06-10T15:56:40.900288Z","shell.execute_reply.started":"2025-06-10T15:56:40.889171Z","shell.execute_reply":"2025-06-10T15:56:40.899285Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Remove Punctuation","metadata":{}},{"cell_type":"code","source":"def remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.901318Z","iopub.execute_input":"2025-06-10T15:56:40.901613Z","iopub.status.idle":"2025-06-10T15:56:40.912247Z","shell.execute_reply.started":"2025-06-10T15:56:40.901570Z","shell.execute_reply":"2025-06-10T15:56:40.911551Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"###  Remove Everything Except A–Z/a–z and Spaces","metadata":{}},{"cell_type":"code","source":"def remove_non_alpha(text):\n    return re.sub(r'[^a-zA-Z\\s]', '', text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.913088Z","iopub.execute_input":"2025-06-10T15:56:40.913466Z","iopub.status.idle":"2025-06-10T15:56:40.925929Z","shell.execute_reply.started":"2025-06-10T15:56:40.913443Z","shell.execute_reply":"2025-06-10T15:56:40.925172Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Tokenization","metadata":{}},{"cell_type":"code","source":"def tokenize(text):\n    return word_tokenize(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.926903Z","iopub.execute_input":"2025-06-10T15:56:40.927172Z","iopub.status.idle":"2025-06-10T15:56:40.938842Z","shell.execute_reply.started":"2025-06-10T15:56:40.927124Z","shell.execute_reply":"2025-06-10T15:56:40.937876Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Remove Stop Words","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\ndef remove_stopwords(tokens):\n    return [word for word in tokens if word not in stop_words and len(word) > 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.939749Z","iopub.execute_input":"2025-06-10T15:56:40.940047Z","iopub.status.idle":"2025-06-10T15:56:40.952489Z","shell.execute_reply.started":"2025-06-10T15:56:40.940021Z","shell.execute_reply":"2025-06-10T15:56:40.951629Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Stemming","metadata":{}},{"cell_type":"code","source":"stemmer = PorterStemmer()\ndef stem_words(tokens):\n    return [stemmer.stem(word) for word in tokens]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.953391Z","iopub.execute_input":"2025-06-10T15:56:40.953727Z","iopub.status.idle":"2025-06-10T15:56:40.967489Z","shell.execute_reply.started":"2025-06-10T15:56:40.953701Z","shell.execute_reply":"2025-06-10T15:56:40.966543Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Lemmatization","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef lemmatize_words(tokens):\n    return [lemmatizer.lemmatize(word, pos='v') for word in tokens]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.970848Z","iopub.execute_input":"2025-06-10T15:56:40.971159Z","iopub.status.idle":"2025-06-10T15:56:40.980256Z","shell.execute_reply.started":"2025-06-10T15:56:40.971116Z","shell.execute_reply":"2025-06-10T15:56:40.979482Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"### Handle Contractions","metadata":{}},{"cell_type":"code","source":"def expand_contractions(text):\n    return contractions.fix(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.981004Z","iopub.execute_input":"2025-06-10T15:56:40.981286Z","iopub.status.idle":"2025-06-10T15:56:40.990947Z","shell.execute_reply.started":"2025-06-10T15:56:40.981266Z","shell.execute_reply":"2025-06-10T15:56:40.990178Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Handle Emojis and Emoticons","metadata":{}},{"cell_type":"code","source":"def handle_emojis(text):\n    return emoji.demojize(text, delimiters=(\" \", \" \"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:40.991816Z","iopub.execute_input":"2025-06-10T15:56:40.992066Z","iopub.status.idle":"2025-06-10T15:56:41.003367Z","shell.execute_reply.started":"2025-06-10T15:56:40.992041Z","shell.execute_reply":"2025-06-10T15:56:41.002587Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Spell Check","metadata":{}},{"cell_type":"code","source":"english_vocab = set(w.lower() for w in words.words())\n\ndef spell_check(word):\n    if word in english_vocab:\n        return word\n    else:\n        matches = get_close_matches(word, english_vocab, n=1, cutoff=0.8)\n        return matches[0] if matches else word","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:41.004537Z","iopub.execute_input":"2025-06-10T15:56:41.004841Z","iopub.status.idle":"2025-06-10T15:56:41.156898Z","shell.execute_reply.started":"2025-06-10T15:56:41.004816Z","shell.execute_reply":"2025-06-10T15:56:41.156271Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### CLEANING FUNCTION","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = to_lowercase(text)\n    text = expand_contractions(text)\n    text = remove_numbers(text)\n    text = remove_punctuation(text)\n    text = handle_emojis(text)\n    text = remove_non_alpha(text)\n    \n    tokens = tokenize(text)\n    tokens = remove_stopwords(tokens)\n    tokens = stem_words(tokens)\n    tokens = lemmatize_words(tokens)\n    tokens = [spell_check(word) for word in tokens]\n    \n    return ' '.join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:41.157691Z","iopub.execute_input":"2025-06-10T15:56:41.157976Z","iopub.status.idle":"2025-06-10T15:56:41.163260Z","shell.execute_reply.started":"2025-06-10T15:56:41.157951Z","shell.execute_reply":"2025-06-10T15:56:41.162460Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Import and Load the ALBERT Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AlbertTokenizer\n\n# Load ALBERT tokenizer (you can use other ALBERT variants too)\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:41.164012Z","iopub.execute_input":"2025-06-10T15:56:41.164271Z","iopub.status.idle":"2025-06-10T15:56:42.296983Z","shell.execute_reply.started":"2025-06-10T15:56:41.164252Z","shell.execute_reply":"2025-06-10T15:56:42.296047Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a7b87f9c9d4ae3bb37ebb6a1d571d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f4b1292b86499a8ae51465f93c131c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d31e0118e124954add793dcca56ef2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9d5b2d691d439097e657829792d151"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"def tokenize_custom(text, max_len=256, pad=True, trunc=True):\n    return tokenizer(\n            text,\n            padding='max_length',        # or 'longest' or False\n            truncation=True,             # or False\n            max_length=128,              # or any custom length (e.g., 256, 512)\n            return_tensors=\"pt\",         # \"pt\" for PyTorch, \"tf\" for TensorFlow, or None\n            add_special_tokens=True,     # Whether to add [CLS] and [SEP] tokens\n            return_attention_mask=True,  # Whether to return the attention mask\n            return_token_type_ids=False, # ALBERT uses token_type_ids for NSP (optional)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.297961Z","iopub.execute_input":"2025-06-10T15:56:42.298252Z","iopub.status.idle":"2025-06-10T15:56:42.303394Z","shell.execute_reply.started":"2025-06-10T15:56:42.298231Z","shell.execute_reply":"2025-06-10T15:56:42.302371Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Load & Clean Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef load_and_clean_data():\n    df = pd.read_csv('/kaggle/input/social-media-sentiments-analysis-dataset')\n    df = df[df['Sentiment'].isin(['Positive', 'Negative'])]  # Drop Neutral\n    df['label'] = df['Sentiment'].map({'Positive': 1, 'Negative': 0})\n    df['cleaned'] = df['Text'].apply(clean_text)\n    return df[['cleaned', 'label']]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.304413Z","iopub.execute_input":"2025-06-10T15:56:42.304766Z","iopub.status.idle":"2025-06-10T15:56:42.320786Z","shell.execute_reply.started":"2025-06-10T15:56:42.304739Z","shell.execute_reply":"2025-06-10T15:56:42.319865Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Prepare Train/Test Split","metadata":{}},{"cell_type":"code","source":"df = load_and_clean_data()\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.321691Z","iopub.execute_input":"2025-06-10T15:56:42.321998Z","iopub.status.idle":"2025-06-10T15:56:42.617889Z","shell.execute_reply.started":"2025-06-10T15:56:42.321974Z","shell.execute_reply":"2025-06-10T15:56:42.616775Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/4206246683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2786784992.py\u001b[0m in \u001b[0;36mload_and_clean_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_clean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/social-media-sentiments-analysis-dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Negative'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Drop Neutral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Positive'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Negative'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/kaggle/input/social-media-sentiments-analysis-dataset'"],"ename":"IsADirectoryError","evalue":"[Errno 21] Is a directory: '/kaggle/input/social-media-sentiments-analysis-dataset'","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"\nX_train, X_test, y_train, y_test = train_test_split(\n    df['cleaned'].tolist(),\n    df['label'].tolist(),\n    test_size=0.2,\n    random_state=42,\n    stratify=df['label']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.618610Z","iopub.status.idle":"2025-06-10T15:56:42.618996Z","shell.execute_reply.started":"2025-06-10T15:56:42.618801Z","shell.execute_reply":"2025-06-10T15:56:42.618818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tokenized Dataset Class","metadata":{}},{"cell_type":"code","source":"df = load_and_clean_data()\nprint(\"First 5 records:\", df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.620401Z","iopub.status.idle":"2025-06-10T15:56:42.620677Z","shell.execute_reply.started":"2025-06-10T15:56:42.620556Z","shell.execute_reply":"2025-06-10T15:56:42.620567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomTextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer):\n        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.621641Z","iopub.status.idle":"2025-06-10T15:56:42.621881Z","shell.execute_reply.started":"2025-06-10T15:56:42.621760Z","shell.execute_reply":"2025-06-10T15:56:42.621770Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build Model & Trainer","metadata":{}},{"cell_type":"code","source":"model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=2)\n\ntrain_dataset = CustomTextDataset(X_train, y_train, tokenizer)\ntest_dataset = CustomTextDataset(X_test, y_test, tokenizer)\n\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=2,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"no\",\n    logging_dir='./logs',\n    logging_steps=100,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.623286Z","iopub.status.idle":"2025-06-10T15:56:42.623513Z","shell.execute_reply.started":"2025-06-10T15:56:42.623407Z","shell.execute_reply":"2025-06-10T15:56:42.623417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.624334Z","iopub.status.idle":"2025-06-10T15:56:42.624596Z","shell.execute_reply.started":"2025-06-10T15:56:42.624482Z","shell.execute_reply":"2025-06-10T15:56:42.624494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluate","metadata":{}},{"cell_type":"code","source":"preds = trainer.predict(test_dataset)\ny_pred = np.argmax(preds.predictions, axis=1)\n\nacc = accuracy_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(\"✅ Accuracy:\", acc)\nprint(\"\\n🧾 Classification Report:\\n\", report)\nprint(\"\\n🧱 Confusion Matrix:\\n\", cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T15:56:42.625831Z","iopub.status.idle":"2025-06-10T15:56:42.626830Z","shell.execute_reply.started":"2025-06-10T15:56:42.626626Z","shell.execute_reply":"2025-06-10T15:56:42.626644Z"}},"outputs":[],"execution_count":null}]}